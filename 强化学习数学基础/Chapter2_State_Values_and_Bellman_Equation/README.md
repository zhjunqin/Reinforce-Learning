# 状态价值（State Values）和贝尔曼方程（Bellman Equation）

## 状态价值（State Values）

为了介绍状态价值先引入一些符号。

考虑一个时间步骤序列 $t = 0, 1, 2, ...$，在时刻 $t$，Agent 处于状态 $ S_t$，并按照策略 $\pi$ 采取了动作 $A_t$。然后转移到下一个状态 $S_{t+1}$，获得的即时奖励是 $R_{t+1}$。这个过程可以简洁的表示为

$$S_t \xrightarrow{A_t}  S_{t+1},R_{t+1}$$

注意，这里的 $S_t,S_{t+1},A_t,R_{t+1}$ 都是随机变量，而且  $S_t,S_{t+1} \in \mathcal{S}$， $A_t \in \mathcal{A(S_t)}$，$R_{t+1} \in \mathcal{R(S_t, A_t)}$

从 $t$ 时刻开始，可以获得一个 state-action-reward 的轨迹：

$$ S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} ... $$

沿着轨迹，获得的折扣回报是从时刻 $t$ 开始的累积折扣奖励：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$$

$\gamma$ 是折扣因子，$\gamma \in (0, 1) $。

注意，这里的 $R_{t+1}, R_{t+2}, ...$ 都是随机变量，因此 $G_t$ 也是随机变量。

由于 $G_t$ 是随机变量，我们可以计算它的期望值：

$$v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]$$

理解为从状态 $s$ 开始，按照策略 $\pi$ 行动，获得的期望回报。

这里 $v_\pi(s)$ 称之为**状态价值函数(state-value function)** 或者 $s$ 的**状态价值(state value)**。

一些重要的说明：
- $v_\pi(s)$ 依赖于 $s$。这是因为它的定义是一个条件期望，条件是 Agent 从 $S_t=s$ 开始。
- $v_\pi(s)$ 依赖于 $\pi$。这是因为轨迹是通过遵循策略 $\pi$ 生成的。对于不同的策略，状态价值可能不同。
- $v_\pi(s)$ 不依赖于 $t$。如果 Agent 在状态空间中移动，$t$ 代表当前时间步。一旦给出策略 $\pi$，其对应的$v_\pi(s)$ 值就确定了，与时刻 $t$ 无关。

### 状态价值与回报之间的关系

当策略和系统模型都是确定的时（采取的策略固定，迁移的状态固定），从某个状态 $s$ 开始总是会有相同的轨迹。在这种情况下，从一个状态 $s$ 开始获得的回报等于该状态的价值。

相比之下，当策略或系统模型中的任何一个都是随机的时，从相同的状态 $s$ 开始可能会生成不同的轨迹。在这种情况下，不同轨迹的回报是不同的，状态价值是这些回报的平均值。

尽管回报可以用来评估策略，但使用状态价值来评估策略更为正式：产生更高状态价值的策略更好。因此，状态价值构成了强化学习中的一个核心概念。

## 贝尔曼方程（Bellman Equation）

接下来推导贝尔曼方程。
首先，我们可以将折扣回报 $G_t$ 重写为：

$$\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \\
&= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ...) \\
&= R_{t+1} + \gamma G_{t+1} \\
\end{aligned}$$

这个等式建立了 $G_t$ 和 $G_{t+1}$之间的关系。表明折扣回报 $G_t$ 可以分解为即时奖励 $R_{t+1}$ 和下一个时刻的折扣回报 $G_{t+1}$ 的加权和。

因此状态价值函数可以被重写为：

$$\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi[G_t|S_t=s] \\
&= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s] \\
&=  \mathbb{E}_\pi[R_{t+1} |S_t=s] +  \gamma \mathbb{E}_\pi[G_{t+1}|S_t=s]
\end{aligned}$$

